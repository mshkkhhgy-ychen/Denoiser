{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "poxUDNOaoywR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmk3zTM9k155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3244467-6e8e-40e4-cb28-c3d5e6170a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -U openmim\n",
        "!mim install mmcv==1.4.0\n",
        "!pip install pytorch_msssim\n",
        "!pip install tensorboardX\n",
        "!pip install thop"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_Sn8FfNWmMeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674eee96-8b17-4052-d3c1-ce4505d4b699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.16.0 in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2024.10.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (2.28.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: openmim in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from openmim) (0.4.6)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.10/dist-packages (from openmim) (0.1.11)\n",
            "Requirement already satisfied: opendatalab in /usr/local/lib/python3.10/dist-packages (from openmim) (0.0.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (2.2.2)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.28.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.4.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.7)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (4.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (3.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.65.2)\n",
            "Requirement already satisfied: openxlab in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.16.0)\n",
            "Requirement already satisfied: filelock~=3.14.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (3.14.0)\n",
            "Requirement already satisfied: oss2~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (2.17.0)\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (24.2)\n",
            "Requirement already satisfied: setuptools~=60.2.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (60.2.0)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/index.html\n",
            "Requirement already satisfied: mmcv==1.4.0 in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv==1.4.0) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv==1.4.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv==1.4.0) (24.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv==1.4.0) (11.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv==1.4.0) (6.0.2)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv==1.4.0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==1.4.0) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==1.4.0) (2.1.0)\n",
            "Requirement already satisfied: pytorch_msssim in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch_msssim) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2024.10.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch_msssim) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch_msssim) (1.3.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (4.25.5)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "import torch.nn.init as init\n",
        "\n",
        "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device = \" + device)\n",
        "if device == 'cpu':\n",
        "    print(\"WARNING: Using CPU will cause slower train times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTGUajK4o7s0",
        "outputId": "5715ff59-f84e-4cc2-d3e1-7690665971ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device = cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Denoiser/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaaT0uTBl64B",
        "outputId": "f803e45b-406b-44be-e6a6-f5a3585f68e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Denoiser/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kaiming_init(module, mode='fan_in', nonlinearity='relu'):\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        init.kaiming_uniform_(module.weight, mode=mode, nonlinearity=nonlinearity)\n",
        "        if module.bias is not None:\n",
        "            init.zeros_(module.bias)\n",
        "\n",
        "def constant_init(module, val=0):\n",
        "    if hasattr(module, 'weight') and module.weight is not None:\n",
        "        init.constant_(module.weight, val)\n",
        "    if hasattr(module, 'bias') and module.bias is not None:\n",
        "        init.constant_(module.bias, val)\n",
        "\n",
        "def last_zero_init(module):\n",
        "    if isinstance(module, nn.Sequential) and len(module) > 0:\n",
        "        constant_init(module[-1], val=0)\n",
        "    elif module is not None:\n",
        "        constant_init(module, val=0)\n"
      ],
      "metadata": {
        "id": "VZQFYmztqqO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 inplanes,\n",
        "                 ratio,\n",
        "                 pooling_type='att',\n",
        "                 fusion_types=('channel_add', )):\n",
        "        super(ContextBlock, self).__init__()\n",
        "        assert pooling_type in ['avg', 'att']\n",
        "        assert isinstance(fusion_types, (list, tuple))\n",
        "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
        "        assert all([f in valid_fusion_types for f in fusion_types])\n",
        "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
        "        # inplanes = 96                 # TEMP HARDCODING INPLANES\n",
        "        self.inplanes = inplanes\n",
        "        self.ratio = ratio\n",
        "        self.planes = int(inplanes * ratio)\n",
        "        self.pooling_type = pooling_type\n",
        "        self.fusion_types = fusion_types\n",
        "        if pooling_type == 'att':\n",
        "            print('INPLANES:', inplanes)\n",
        "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
        "            self.softmax = nn.Softmax(dim=2)\n",
        "        else:\n",
        "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        if 'channel_add' in fusion_types:\n",
        "            self.channel_add_conv = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
        "                nn.LayerNorm([self.planes, 1, 1]),\n",
        "                nn.ReLU(inplace=True),  # yapf: disable\n",
        "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
        "        else:\n",
        "            self.channel_add_conv = None\n",
        "        if 'channel_mul' in fusion_types:\n",
        "            self.channel_mul_conv = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
        "                nn.LayerNorm([self.planes, 1, 1]),\n",
        "                nn.ReLU(inplace=True),  # yapf: disable\n",
        "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
        "        else:\n",
        "            self.channel_mul_conv = None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.pooling_type == 'att':\n",
        "            kaiming_init(self.conv_mask, mode='fan_in')\n",
        "            self.conv_mask.inited = True\n",
        "\n",
        "        if self.channel_add_conv is not None:\n",
        "            last_zero_init(self.channel_add_conv)\n",
        "        if self.channel_mul_conv is not None:\n",
        "            last_zero_init(self.channel_mul_conv)\n",
        "\n",
        "    def spatial_pool(self, x):\n",
        "        print('gc spatial pooling. xsize:', x.size())\n",
        "        #x = x.permute(2,0,1)\n",
        "        batch, channel, height, width = x.size()\n",
        "        L = height * width\n",
        "        #batch, channel, L = x.size()\n",
        "        if self.pooling_type == 'att':\n",
        "            print('self.pooling_type==att. inside if statement')\n",
        "            input_x = x\n",
        "            # [N, C, H * W]\n",
        "            input_x = input_x.view(batch, channel, L)\n",
        "            # [N, 1, C, H * W]\n",
        "            input_x = input_x.unsqueeze(1)\n",
        "            # [N, 1, H, W]\n",
        "            context_mask = self.conv_mask(x)\n",
        "            # [N, 1, H * W]\n",
        "            context_mask = context_mask.view(batch, 1, L)\n",
        "            # [N, 1, H * W]\n",
        "            context_mask = self.softmax(context_mask)\n",
        "            # [N, 1, H * W, 1]\n",
        "            context_mask = context_mask.unsqueeze(-1)\n",
        "            # [N, 1, C, 1]\n",
        "            context = torch.matmul(input_x, context_mask)\n",
        "            # [N, C, 1, 1]\n",
        "            context = context.view(batch, channel, 1, 1)\n",
        "        else:\n",
        "            # [N, C, 1, 1]\n",
        "            context = self.avg_pool(x)\n",
        "\n",
        "        return context\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('gc forward.')\n",
        "        # [N, C, 1, 1]\n",
        "        context = self.spatial_pool(x)\n",
        "\n",
        "        out = x\n",
        "        if self.channel_mul_conv is not None:\n",
        "            # [N, C, 1, 1]\n",
        "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
        "            out = out * channel_mul_term\n",
        "        if self.channel_add_conv is not None:\n",
        "            # [N, C, 1, 1]\n",
        "            channel_add_term = self.channel_add_conv(context)\n",
        "            out = out + channel_add_term\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "yfYo8srMqePy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from model.SUNet_detail_gcn import ContextBlock\n",
        "context_block = ContextBlock(\n",
        "    inplanes= 64,\n",
        "    ratio=0.25,\n",
        "    pooling_type='att',\n",
        "    fusion_types=('channel_add', 'channel_mul')\n",
        ")\n",
        "batch_size = 4\n",
        "channels = 64\n",
        "height, width = 32, 32\n",
        "input_tensor = torch.rand(batch_size, channels, height, width)\n",
        "output_tensor = context_block(input_tensor)\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "print(\"Output shape:\", output_tensor.shape)\n",
        "# print(\"Diff:\", output_tensor[0] - input_tensor[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-0GBqImLj8P",
        "outputId": "475f2eae-1471-477c-ce67-3040759e9ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPLANES: 64\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([4, 64, 32, 32])\n",
            "self.pooling_type==att. inside if statement\n",
            "Input shape: torch.Size([4, 64, 32, 32])\n",
            "Output shape: torch.Size([4, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StridedConvolutionDownsampling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d, stride = 2, kernel_size = 3, padding = 1):\n",
        "        super().__init__()\n",
        "        self.conv_down = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, stride= stride, padding=padding\n",
        "        )\n",
        "        self.norm = norm_layer(out_channels)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_down(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6nvuFsvk71FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "in_channels = 30\n",
        "out_channels = 50\n",
        "input_height, input_width = 32, 32  # Input dimensions (HxW)\n",
        "\n",
        "# Create the module\n",
        "model = StridedConvolutionDownsampling(in_channels, out_channels)\n",
        "\n",
        "# Create a random input tensor with shape (batch_size, channels, height, width)\n",
        "batch_size = 8\n",
        "input_tensor = torch.randn(batch_size, in_channels, input_height, input_width)\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "\n",
        "# Expected output shape\n",
        "stride = 2\n",
        "expected_height = input_height // stride  # Stride=2 halves the spatial dimensions\n",
        "expected_width = input_width // stride\n",
        "expected_shape = (batch_size, out_channels, expected_height, expected_width)\n",
        "\n",
        "# Test the output shape\n",
        "assert output.shape == expected_shape, f\"Expected {expected_shape}, but got {output.shape}\"\n",
        "print(\"Test passed! Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ-nuzQYtVbQ",
        "outputId": "5228a2cc-ca09-4b78-9645-3d3312738ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 30, 32, 32])\n",
            "Test passed! Output shape: torch.Size([8, 50, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# global context basic layer\n",
        "class GlobalContextBasicLayer(nn.Module):\n",
        "    def __init__(self, dim, ratio, pooling_type, fusion_types,\n",
        "                 depth, input_resolution, norm_layer=nn.LayerNorm,\n",
        "                 downsample= None, use_checkpoint=False, stride = 2, out_channels = 50):\n",
        "        print('we making a gc basic layer')\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.ratio = ratio\n",
        "        self.pooling_type = pooling_type\n",
        "        self.fusion_types = fusion_types\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.stride = stride\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # create a series of ContextBlocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ContextBlock(inplanes=dim, ratio=ratio, pooling_type=pooling_type,\n",
        "                         fusion_types=fusion_types) for _ in range(depth)\n",
        "        ])\n",
        "        print(\"The input_resolution is:\", input_resolution)\n",
        "        print(\"After context blocks, the shape is:\", ())\n",
        "        # downsample/patch merging layer\n",
        "        if downsample is not None:\n",
        "            self.downsample = StridedConvolutionDownsampling(in_channels = dim, out_channels= out_channels, norm_layer= nn.BatchNorm2d, stride = stride)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('we going forward in a gc basic layer')\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TtcEkXs6mvQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global context basic layer test\n",
        "layer = GlobalContextBasicLayer(\n",
        "    dim=30,\n",
        "    ratio=0.25,\n",
        "    pooling_type='avg',\n",
        "    fusion_types=('channel_add',),\n",
        "    depth=3,\n",
        "    input_resolution=(32, 32),\n",
        "    norm_layer=nn.LayerNorm,\n",
        "    downsample = StridedConvolutionDownsampling,\n",
        "    # downsample= None,\n",
        "    use_checkpoint=False\n",
        ")\n",
        "\n",
        "# Input tensor: batch size = 8, channels = 30, height = 32, width = 32\n",
        "x = torch.randn(8, 30, 32, 32)\n",
        "output = layer(x)\n",
        "# output = layer(output)\n",
        "# output = layer(output)\n",
        "\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zezr6Og97kea",
        "outputId": "9fd4aaf8-974c-416a-f312-427901d66e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we making a gc basic layer\n",
            "The input_resolution is: (32, 32)\n",
            "After context blocks, the shape is: ()\n",
            "we going forward in a gc basic layer\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([8, 30, 32, 32])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([8, 30, 32, 32])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([8, 30, 32, 32])\n",
            "Output shape: torch.Size([8, 50, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dual up-sample\n",
        "# class UpSample(nn.Module):\n",
        "#     def __init__(self, input_resolution, in_channels, scale_factor):\n",
        "#         super(UpSample, self).__init__()\n",
        "#         print('dual upsample')\n",
        "#         self.input_resolution = input_resolution\n",
        "#         self.factor = scale_factor\n",
        "\n",
        "\n",
        "#         if self.factor == 2:\n",
        "#             self.conv = nn.Conv2d(in_channels, in_channels//2, 1, 1, 0, bias=False)\n",
        "#             self.up_p = nn.Sequential(nn.Conv2d(in_channels, 2*in_channels, 1, 1, 0, bias=False),\n",
        "#                                       nn.PReLU(),\n",
        "#                                       nn.PixelShuffle(scale_factor),\n",
        "#                                       nn.Conv2d(in_channels//2, in_channels//2, 1, stride=1, padding=0, bias=False))\n",
        "\n",
        "#             self.up_b = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1, 1, 0),\n",
        "#                                       nn.PReLU(),\n",
        "#                                       nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False),\n",
        "#                                       nn.Conv2d(in_channels, in_channels // 2, 1, stride=1, padding=0, bias=False))\n",
        "#         elif self.factor == 4:\n",
        "#             self.conv = nn.Conv2d(2*in_channels, in_channels, 1, 1, 0, bias=False)\n",
        "#             self.up_p = nn.Sequential(nn.Conv2d(in_channels, 16 * in_channels, 1, 1, 0, bias=False),\n",
        "#                                       nn.PReLU(),\n",
        "#                                       nn.PixelShuffle(scale_factor),\n",
        "#                                       nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0, bias=False))\n",
        "\n",
        "#             self.up_b = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1, 1, 0),\n",
        "#                                       nn.PReLU(),\n",
        "#                                       nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False),\n",
        "#                                       nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0, bias=False))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         x: B, L = H*W, C\n",
        "#         \"\"\"\n",
        "#         print('dual upsample forward')\n",
        "#         if type(self.input_resolution) == int:\n",
        "#             H = self.input_resolution\n",
        "#             W = self.input_resolution\n",
        "\n",
        "#         elif type(self.input_resolution) == tuple:\n",
        "#             H, W = self.input_resolution\n",
        "\n",
        "#         B, L, C = x.shape\n",
        "#         x = x.view(B, H, W, C)  # B, H, W, C\n",
        "#         x = x.permute(0, 3, 1, 2)  # B, C, H, W\n",
        "#         x_p = self.up_p(x)  # pixel shuffle\n",
        "#         x_b = self.up_b(x)  # bilinear\n",
        "#         out = self.conv(torch.cat([x_p, x_b], dim=1))\n",
        "#         out = out.permute(0, 2, 3, 1)  # B, H, W, C\n",
        "#         if self.factor == 2:\n",
        "#             out = out.view(B, -1, C // 2)\n",
        "\n",
        "#         return out"
      ],
      "metadata": {
        "id": "rxHUCNKO9-aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    def __init__(self, input_resolution, in_channels, scale_factor):\n",
        "        super(UpSample, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.upsample = nn.Upsample(scale_factor=self.scale_factor, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure the input is in the (B, C, H, W) format\n",
        "        if x.dim() != 4:\n",
        "            raise ValueError(f\"Expected input to have 4 dimensions (B, C, H, W), but got {x.shape}\")\n",
        "        # Apply upsampling directly\n",
        "        return self.upsample(x)\n"
      ],
      "metadata": {
        "id": "3HN3irFMxMc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global context basic up layer\n",
        "# from model.SUNet_detail_gcn import UpSample\n",
        "class GlobalContextBasicUpLayer(nn.Module):\n",
        "    def __init__(self, dim, ratio, pooling_type, fusion_types,\n",
        "                 depth, input_resolution, norm_layer=nn.LayerNorm,\n",
        "                 upsample=None, use_checkpoint=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.ratio = ratio\n",
        "        self.pooling_type = pooling_type\n",
        "        self.fusion_types = fusion_types\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.upsample = upsample\n",
        "\n",
        "        # create a series of ContextBlocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ContextBlock(inplanes=dim, ratio=ratio, pooling_type=pooling_type,\n",
        "                         fusion_types=fusion_types) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # upsample\n",
        "        if upsample is not None:\n",
        "            self.upsample = UpSample(input_resolution, in_channels=dim, scale_factor=2)\n",
        "        else:\n",
        "            self.upsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "            if self.upsample is not None:\n",
        "                x = self.upsample(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eKGINetV9Kt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global context basic layer up test\n",
        "layer = GlobalContextBasicUpLayer(\n",
        "    dim=50,\n",
        "    ratio=0.25,\n",
        "    pooling_type='avg',\n",
        "    fusion_types=('channel_add',),\n",
        "    depth=3,\n",
        "    input_resolution=(16, 16),\n",
        "    upsample=None,\n",
        "    use_checkpoint=False\n",
        ")\n",
        "\n",
        "# Input tensor: batch size = 8, channels = 50, height = 16, width = 16\n",
        "x = torch.randn(8, 50, 16, 16)\n",
        "output = layer(x)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ekdfY1gBUPH",
        "outputId": "57d0b2a0-e433-49f1-974f-69cde892f640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([8, 50, 16, 16])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([8, 50, 16, 16])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([8, 50, 16, 16])\n",
            "Output shape: torch.Size([8, 50, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if self.final_upsample == \"Dual up-sample\":\n",
        "up = UpSample(input_resolution=(16, 16),\n",
        "                    in_channels= 50, scale_factor=4)\n",
        "def test_upsample():\n",
        "    # Parameters\n",
        "    input_resolution = (16, 16)\n",
        "    in_channels = 50\n",
        "    scale_factor = 4\n",
        "    batch_size = 2\n",
        "\n",
        "    # Create a random input tensor with shape [batch_size, in_channels, height, width]\n",
        "    input_tensor = torch.rand(batch_size, in_channels, input_resolution[0], input_resolution[1])\n",
        "\n",
        "    # Initialize the UpSample layer\n",
        "    upsample = UpSample(input_resolution=input_resolution, in_channels=in_channels, scale_factor=scale_factor)\n",
        "\n",
        "    # Pass the input through the UpSample layer\n",
        "    output_tensor = upsample(input_tensor)\n",
        "\n",
        "    # Compute the expected output shape\n",
        "    expected_height = input_resolution[0] * scale_factor\n",
        "    expected_width = input_resolution[1] * scale_factor\n",
        "    expected_shape = (batch_size, in_channels, expected_height, expected_width)\n",
        "\n",
        "    # Assertions to verify correctness\n",
        "    assert output_tensor.shape == expected_shape, f\"Expected shape {expected_shape}, but got {output_tensor.shape}\"\n",
        "    print(f\"Test passed! Output shape: {output_tensor.shape}\")\n",
        "\n",
        "# Run the test case\n",
        "test_upsample()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7lPcFzHwrt-",
        "outputId": "50c5221a-cf53-45a7-dbb7-0fc99004a678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed! Output shape: torch.Size([2, 50, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if self.final_upsample == \"Dual up-sample\":\n",
        "up = UpSample(input_resolution=(16, 16),\n",
        "                    in_channels= 50, scale_factor=4)\n",
        "\n",
        "# Define the Conv2d layer\n",
        "output_layer = nn.Conv2d(\n",
        "    in_channels=50, out_channels=30, kernel_size=3, stride=1, padding=1, bias=False\n",
        ")\n",
        "\n",
        "# Create a dummy input tensor (batch_size, in_channels, height, width)\n",
        "input_tensor = torch.randn(8, 50, 16, 16)  # Example shape: (batch size 1, 50 channels, 64x64)\n",
        "\n",
        "# Pass the input tensor through the Conv2d layer\n",
        "output_tensor = output_layer(input_tensor)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnmCNUBp-gjA",
        "outputId": "08b457bd-b890-4d8b-afad-c7dabb5f9761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dual upsample\n",
            "torch.Size([8, 30, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalNet(nn.Module):\n",
        "    def __init__(self, img_size=224, in_chans=3, out_chans=3,\n",
        "                 norm_layer=nn.LayerNorm, ape=False,\n",
        "                 use_checkpoint=False, final_upsample=\"Dual up-sample\",\n",
        "                 ratio=0.8, pooling_type='avg', gc_layer=3, downsample = False, stride = 2, downsample_out_chans = 6, **kwargs):\n",
        "        super(GlobalNet, self).__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.in_chans = in_chans\n",
        "        self.out_chans = out_chans\n",
        "        self.ape = ape\n",
        "        self.final_upsample = final_upsample\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.ratio = ratio\n",
        "        self.pooling_type = pooling_type\n",
        "        self.gc_layers = gc_layer\n",
        "        self.downsample = downsample\n",
        "        self.downsample_out_chans = downsample_out_chans\n",
        "        self.stride = stride\n",
        "        # Calculate embedding dimension based on image size and channels\n",
        "        # self.embed_dim = in_chans * img_size * img_size\n",
        "        # self.num_features = self.embed_dim\n",
        "\n",
        "        # Build encoder and bottleneck layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        # for i_layer in range(self.gc_layers):\n",
        "        layer = GlobalContextBasicLayer(\n",
        "            dim=self.in_chans,\n",
        "            ratio=ratio,\n",
        "            pooling_type=pooling_type,\n",
        "            fusion_types=('channel_add',),\n",
        "            depth=3,\n",
        "            input_resolution=(img_size, img_size),\n",
        "            norm_layer=norm_layer,\n",
        "            downsample= StridedConvolutionDownsampling,\n",
        "            use_checkpoint=self.use_checkpoint,\n",
        "            stride = self.stride,\n",
        "            out_channels=self.downsample_out_chans\n",
        "        )\n",
        "        self.layers.append(layer)\n",
        "        print(\"Finish Encoder Part!\")\n",
        "\n",
        "        # Build decoder layers\n",
        "        self.layers_up = nn.ModuleList()\n",
        "        # self.concat_back_dim = nn.ModuleList()\n",
        "        # for i_layer in range(self.gc_layers):\n",
        "        # if i_layer == 0:\n",
        "        layer_up = UpSample(\n",
        "            input_resolution=(img_size // self.stride, img_size // self.stride),\n",
        "            in_channels= self.downsample_out_chans,\n",
        "            scale_factor= stride\n",
        "        )\n",
        "        self.layers_up.append(layer_up)\n",
        "        # else:\n",
        "        layer_up = GlobalContextBasicUpLayer(\n",
        "            dim= self.downsample_out_chans,\n",
        "            ratio = ratio,\n",
        "            pooling_type=pooling_type,\n",
        "            fusion_types=('channel_add',),\n",
        "            depth=3,\n",
        "            input_resolution=(img_size, img_size),\n",
        "            norm_layer=nn.LayerNorm,\n",
        "            upsample=None,\n",
        "            use_checkpoint=self.use_checkpoint\n",
        "        )\n",
        "        print(\"Finish Decoder Part!\")\n",
        "\n",
        "        self.layers_up.append(layer_up)\n",
        "        # Normalization layers\n",
        "        self.norm = norm_layer(self.downsample_out_chans)\n",
        "        self.norm_up = norm_layer(self.downsample_out_chans)\n",
        "\n",
        "        # # Final upsample layers\n",
        "        if self.final_upsample == \"Dual up-sample\":\n",
        "            self.up = UpSample(\n",
        "                input_resolution=(img_size, img_size),\n",
        "                in_channels=self.downsample_out_chans,\n",
        "                scale_factor=2\n",
        "            )\n",
        "            self.output = nn.Conv2d(\n",
        "                in_channels=self.downsample_out_chans,\n",
        "                out_channels=self.out_chans,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "                bias=False\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder part\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)  # Pass through each encoder layer\n",
        "        print(\"Finish Forward Encoder Part!\")\n",
        "\n",
        "        # Decoder part\n",
        "        for layer_up in self.layers_up:\n",
        "            x = layer_up(x)  # Pass through each decoder layer\n",
        "            print(\"Finish One round!\")\n",
        "        print(\"Finish Forward Decoder Part!\")\n",
        "\n",
        "        # Final upsample\n",
        "        if self.final_upsample == \"Dual up-sample\":\n",
        "            x = self.up(x)  # Perform the final upsampling\n",
        "            x = self.output(x)  # Project to the desired output channels\n",
        "        print(\"Finish Forward Upsample Part!\")\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "8CY1jf79hOYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = GlobalNet(img_size=224, in_chans=3, out_chans=3, stride=2)\n",
        "\n",
        "# Create a dummy input tensor (batch_size, channels, height, width)\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Pass the input through the model\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Output shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc8QFgfs1Ghd",
        "outputId": "4dfa3441-f299-47d7-c8d4-c75295b41c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we making a gc basic layer\n",
            "The input_resolution is: (224, 224)\n",
            "After context blocks, the shape is: ()\n",
            "Finish Encoder Part!\n",
            "Finish Decoder Part!\n",
            "we going forward in a gc basic layer\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([1, 3, 224, 224])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([1, 3, 224, 224])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([1, 3, 224, 224])\n",
            "Finish Forward Encoder Part!\n",
            "Finish One round!\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([1, 6, 224, 224])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([1, 6, 224, 224])\n",
            "gc forward.\n",
            "gc spatial pooling. xsize: torch.Size([1, 6, 224, 224])\n",
            "Finish One round!\n",
            "Finish Forward Decoder Part!\n",
            "Finish Forward Upsample Part!\n",
            "Output shape: torch.Size([1, 3, 448, 448])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1Bj-bA_1eb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}