{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install timm\n",
    "!pip install thop\n",
    "!pip install mmcv==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.SUNet_detail import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(4, 3, 256, 256)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 64, 256, 256])\n",
      "Output shape: torch.Size([4, 64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "feature_map = conv(img)  # Shape: (4, 64, 256, 256)\n",
    "\n",
    "context_block = ContextBlock(\n",
    "    inplanes=64,\n",
    "    ratio=0.25,\n",
    "    pooling_type='att',\n",
    "    fusion_types=('channel_add', 'channel_mul')\n",
    ")\n",
    "output = context_block(feature_map)\n",
    "print(\"Input shape:\", feature_map.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StridedConvolutionDownsampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.conv_down = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=2, padding=1\n",
    "        )\n",
    "        self.norm = norm_layer(out_channels)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_down(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([4, 64, 256, 256])\n",
      "x_downsampled shape: torch.Size([4, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = output\n",
    "downsample = StridedConvolutionDownsampling(64, 128)\n",
    "x_downsampled = downsample(x)\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"x_downsampled shape:\", x_downsampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDownsampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
    "        super(AttentionDownsampling, self).__init__()\n",
    "        # Channel attention mechanism\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global average pooling\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // 4, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        # Residual connection\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=stride\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_map = self.channel_attention(x)  # (B, 1, H, W)\n",
    "        x_attended = x * attention_map\n",
    "        x_downsampled = self.conv(x_attended)\n",
    "        res = self.residual(x)\n",
    "        return F.relu(x_downsampled + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 256, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([4, 64, 256, 256])\n",
      "x_downsampled shape: torch.Size([4, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "x = output\n",
    "downsample = AttentionDownsampling(64, 128)\n",
    "x_downsampled = downsample(x)\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"x_downsampled shape:\", x_downsampled.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
